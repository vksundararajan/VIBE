<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ai on VIBE</title><link>https://vksundararajan.github.io/VIBE/tags/ai/</link><description>Recent content in Ai on VIBE</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Wed, 24 Sep 2025 23:28:22 -0400</lastBuildDate><atom:link href="https://vksundararajan.github.io/VIBE/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Kestrel: LLM-Powered Cybersecurity Research Assistant Using RAG</title><link>https://vksundararajan.github.io/VIBE/blog/kestrel/</link><pubDate>Wed, 24 Sep 2025 23:28:22 -0400</pubDate><guid>https://vksundararajan.github.io/VIBE/blog/kestrel/</guid><description>&lt;p>Agentic AI is changing the way we design intelligent systems. Instead of treating an LLM as a single-shot text generator, Agentic AI frameworks let us structure reasoning, retrieval, and decision-making in a way that mimics how humans research and solve problems.&lt;/p>
&lt;p>&lt;mark>Kestrel&lt;/mark> is a project where I applied this approach to a &lt;mark>cybersecurity domain&lt;/mark>. The goal was simple: create a research assistant that can query vulnerability datasets, reason over the results, and generate reliable, context-grounded answers. Under the hood, it combines LLMs, ChromaDB, LangChain, and JSON-based datasets into a Retrieval-Augmented Generation (RAG) pipeline that is configurable and extensible.&lt;/p></description></item><item><title>Behind the Prompts: What I Learned from Engineering LLM Prompts</title><link>https://vksundararajan.github.io/VIBE/blog/behind-the-prompts-what-i-learned-from-engineering-llm-prompts/</link><pubDate>Fri, 16 May 2025 08:55:40 -0400</pubDate><guid>https://vksundararajan.github.io/VIBE/blog/behind-the-prompts-what-i-learned-from-engineering-llm-prompts/</guid><description>&lt;p>Over the last 2 days, I’ve dived deep into &lt;mark>Prompt Engineering&lt;/mark> — not the surface-level prompt crafting, but the core architecture of how LLMs interpret, generate, and optimise responses based on prompt structure and configuration.&lt;/p>
&lt;p>What started as a casual exploration through &lt;strong>Google’s Prompt Engineering Essentials&lt;/strong> turned into an unexpectedly technical journey that bridged NLP design, token sampling, inference configuration, and agent-based reasoning.&lt;/p>
&lt;p>Let’s break it down.&lt;/p>
&lt;h2 id="llms-are-not-magical--theyre-token-predictors">LLMs Are Not Magical — They&amp;rsquo;re Token Predictors&lt;/h2>
&lt;p>Large Language Models, whether GPT, Gemini, Claude, or Llama, are not creative writers — they are statistical machines that &lt;strong>predict the next token given the previous context&lt;/strong>. That’s all.&lt;/p></description></item></channel></rss>